# -*- coding: utf-8 -*-
"""Feature_Engineering_LAB1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuyWaJ7btUQYuntlDGWUebt9nCm54saU

# Importing Needed Packages
"""

# Commented out IPython magic to ensure Python compatibility.
## Data Analysis Phase
## MAin aim is to understand more about the data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
## Display all the columns of the dataframe

pd.pandas.set_option('display.max_columns',None)

"""# Uploading Datasets"""

train_dataset=pd.read_csv('train.csv')
valid_dataset=pd.read_csv('valid.csv')
test_dataset=pd.read_csv('test.csv')
## print shape of dataset with rows and columns
print(train_dataset.shape)
print(valid_dataset.shape)
print(test_dataset.shape)

## print the top5 records
train_dataset.head()
valid_dataset.head()

"""# Check for Missing Values"""

# Step 1: Make a list of features with missing values, excluding the last 4 columns
features_with_na = [feature for feature in train_dataset.columns[:-4] if train_dataset[feature].isnull().sum() > 1]

# Step 2: Check if there are any features with missing values
if not features_with_na:
    print("No missing values found in the selected features.")
else:
    # Print the feature name and the percentage of missing values
    for feature in features_with_na:
        print(feature, np.round(train_dataset[feature].isnull().mean(), 4), ' % missing values')

"""If all features have numerical data types, it will print "All features have numerical values." Otherwise, it will print "Not all features have numerical values."""

# Exclude the last 4 columns (labels )from checking
columns_to_check = train_dataset.columns[:-4]

# Check if all selected features have numerical values
all_features_numerical = all(train_dataset[feature].dtype != 'O' for feature in columns_to_check)

if all_features_numerical:
    print("All selected features have numerical values.")
else:
    print("Not all selected features have numerical values.")

"""# Plotting Missing Values in Labels"""

# Separate features and labels
features = train_dataset.iloc[:, :256]
labels = train_dataset.iloc[:, -4:]

# Calculate missing values in labels
missing_values = labels.isnull().sum()

# Plot missing values
plt.figure(figsize=(10, 6))
sns.barplot(x=missing_values.index, y=missing_values.values)
plt.title('Missing Values in Labels')
plt.xlabel('Label Columns')
plt.ylabel('Number of Missing Values')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# Representation

**Labels** and **Features**
"""

LABELS = [f'label_{i}' for i in range(1, 5)]
FEATURES = [f'feature_{i}' for i in range(1, 257)]

"""# Preprocessing Techniques
In scikit-learn (sklearn), the StandardScaler is a preprocessing technique used to standardize features by removing the mean and scaling them to unit variance. This can be important for some machine learning algorithms that are sensitive to the scale of input features.
"""

from sklearn.preprocessing import StandardScaler
X_train_scaled={}
X_valid_scaled={}
Y_train_scaled={}
Y_valid_scaled={}
X_test_scaled={}
Y_test_scaled={}

for target_Label in LABELS:
    tr_d = train_dataset[train_dataset['label_2'].notna()] if target_Label == 'label_2' else train_dataset
    val_d = valid_dataset[valid_dataset['label_2'].notna()] if target_Label == 'label_2' else valid_dataset
    test_d= test_dataset
    scaler = StandardScaler()

    # Fit the scaler on the training data and transform both training and validation data
    X_train_scaled[target_Label]  = pd.DataFrame(scaler.fit_transform(tr_d.drop(LABELS, axis=1)), columns=FEATURES)
    Y_train_scaled[target_Label] =  tr_d[target_Label]
    X_valid_scaled[target_Label]  = pd.DataFrame(scaler.transform(val_d.drop(LABELS, axis=1)), columns=FEATURES)
    Y_valid_scaled[target_Label] =  val_d[target_Label]
    X_test_scaled[target_Label] = pd.DataFrame(scaler.fit_transform(test_d.drop(LABELS, axis=1)), columns=FEATURES)
    Y_test_scaled[target_Label] = test_d[target_Label]

"""# Label 1

## Model Training
"""

from sklearn import svm
classification=svm.SVC(kernel='linear')
classification.fit(X_train_scaled['label_1'],Y_train_scaled['label_1'])
y_prediction = classification.predict(X_valid_scaled['label_1'])
y_pred_test_before = classification.predict(X_test_scaled['label_1'])

print('Predicted labels before feature engineering:', y_pred_test_before)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

"""## Applying Feature Selection Techniques

### Univariate feature selection

Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.
"""

from sklearn.feature_selection import SelectKBest,f_classif
selector = SelectKBest(f_classif,k=75)
X_new_f1 = selector.fit_transform(X_train_scaled['label_1'],Y_train_scaled['label_1'])
print ('New Shape according to the feature selection technique 1', X_new_f1.shape )

"""Then try to train the model with new feature count and see the difference.



---




"""

classification=svm.SVC(kernel='linear')
classification.fit(X_new_f1,Y_train_scaled['label_1'])
y_prediction = classification.predict(selector.transform(X_valid_scaled['label_1']))
y_test_prediction = classification.predict(selector.transform(X_test_scaled['label_1']))
print(y_test_prediction)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

print ('precision:',metrics.precision_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))
print ('accuracy: ',metrics.accuracy_score( Y_valid_scaled['label_1'] ,y_prediction))
print ('recall:', metrics.recall_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

"""According to this feature selection method  to streamline model managed to reduce the feature count from 256 to 100. However, this reduction was accompanied by a slight dip in the model's accuracy. This decline can be traced back to the careful selection of pertinent features and the omission of less impactful ones during the feature selection process. Unfortunately, this might have unintentionally led to the exclusion of certain information that was contributing to the original model's accuracy.

The primary goal of feature selection is to refine the model by excluding irrelevant or redundant features, but it's vital to acknowledge that the interactions between features also play a pivotal role in the model's performance. By decreasing the feature count, I may have inadvertently removed critical interactions that were essential for accurate predictions.

Another important consideration is the possibility of noise or unnecessary complexity introduced by some features. The removal of such features was aimed at enhancing generalization, which is particularly advantageous when dealing with extensive sets of features. However, it's crucial to strike a balance, as an excessive removal of features could lead the model to oversimplify and miss the intricate nuances in the data, ultimately resulting in reduced accuracy.

### sklearn.decomposition.PCA

Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.88,svd_solver='full')
pca.fit(X_train_scaled['label_1'])
x_train_transformation= pd.DataFrame(pca.transform(X_train_scaled['label_1']))
x_valid_transformation= pd.DataFrame(pca.transform(X_valid_scaled['label_1']))
x_test_transformation= pd.DataFrame(pca.transform(X_test_scaled['label_1']))
x_test_transformation_pass= pca.transform(X_test_scaled['label_1'])
print ('New Shape according to the feature selection technique 1', x_train_transformation.shape )

"""Then try to train the model with new feature count and see the difference."""

classification=svm.SVC(kernel='linear')
classification.fit(x_train_transformation,Y_train_scaled['label_1'])
y_prediction = classification.predict(x_valid_transformation)
y_pred_test_after = classification.predict(x_test_transformation)
print('Predicted labels after feature engineering:', y_pred_test_after)

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test_before,
    'Predicted labels after feature engineering': y_pred_test_after,
    'No. of new features': x_test_transformation_pass.shape[1]
})


for i in range(x_test_transformation_pass.shape[1]):
  output_df[f'New feature {i+1}'] = x_test_transformation_pass[:, i]

output_df.head()

# Save the DataFrame to the specified CSV file path
output_df.to_csv(f"190282X_label_1.csv", index=False)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_1'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

print ("precesion:",metrics.precision_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))
print ("accracy: ",metrics.accuracy_score( Y_valid_scaled['label_1'] ,y_prediction))
print ("recall: ",metrics.recall_score( Y_valid_scaled['label_1'] ,y_prediction,average='weighted'))

"""# Label 2

## Model Training
"""

from sklearn.neighbors import KNeighborsClassifier


# Create a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed

# Fit the KNN classifier to the training data
knn_classifier.fit(X_train_scaled['label_2'], Y_train_scaled['label_2'])

# Make predictions using the KNN classifier
y_prediction = knn_classifier.predict(X_valid_scaled['label_2'])
y_pred_test_before = knn_classifier.predict(X_test_scaled['label_2'])
print('Predicted labels before feature engineering:', y_pred_test_before)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_2'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_2'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))

"""## Applying Feature Selection Techniques

### sklearn.decomposition.PCA

Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.75,svd_solver='full')
pca.fit(X_train_scaled['label_2'])
x_train_transformation= pd.DataFrame(pca.transform(X_train_scaled['label_2']))
x_valid_transformation= pd.DataFrame(pca.transform(X_valid_scaled['label_2']))
x_test_transformation= pca.transform(X_test_scaled['label_2'])
print ('New Shape according to the feature selection technique 1', x_train_transformation.shape )

"""Then try to train the model with new feature count and see the difference."""

from sklearn.neighbors import KNeighborsClassifier



# Create a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed

# Fit the KNN classifier to the training data
knn_classifier.fit(x_train_transformation, Y_train_scaled['label_2'])

# Make predictions using the KNN classifier
y_prediction = knn_classifier.predict(x_valid_transformation)

y_pred_test_after = knn_classifier.predict(x_test_transformation)
print('Predicted labels after feature engineering:', y_pred_test_after)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_2'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_2'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))

print ("precision",metrics.precision_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))
print ("accuracy",metrics.accuracy_score( Y_valid_scaled['label_2'] ,y_prediction))
print ("recall",metrics.recall_score( Y_valid_scaled['label_2'] ,y_prediction,average='weighted'))

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test_before,
    'Predicted labels after feature engineering': y_pred_test_after,
    'No. of new features': x_test_transformation.shape[1]
})


for i in range(x_test_transformation.shape[1]):
  output_df[f'New feature {i+1}'] = x_test_transformation[:, i]

output_df.head()

output_df.to_csv(f"190282X_label_2.csv", index=False)

"""# Label 3

## Model Training
"""

from sklearn import svm
classification=svm.SVC(kernel='linear')
classification.fit(X_train_scaled['label_3'],Y_train_scaled['label_3'])
y_prediction = classification.predict(X_valid_scaled['label_3'])
y_pred_test_before = classification.predict(X_test_scaled['label_3'])
print('Predicted labels before feature engineering:', y_pred_test_before)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_3'],y_prediction))

print (metrics.accuracy_score( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_3'] ,y_prediction))

"""## Applying Feature Selection Techniques

### Univariate feature selection

Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.
"""

from sklearn.feature_selection import SelectKBest,f_classif
selector = SelectKBest(f_classif,k=10)
X_new_f1 = selector.fit_transform(X_train_scaled['label_3'],Y_train_scaled['label_3'])
print ('New Shape according to the feature selection technique 1', X_new_f1.shape )

"""Then try to train the model with new feature count and see the difference.



---




"""

classification=svm.SVC(kernel='linear')
classification.fit(X_new_f1,Y_train_scaled['label_3'])
y_prediction = classification.predict(selector.transform(X_valid_scaled['label_3']))
y_test_prediction = classification.predict(selector.transform(X_test_scaled['label_3']))
print(y_test_prediction)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

print('precision: ',metrics.precision_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))
print('accuracy: ',metrics.accuracy_score( Y_valid_scaled['label_3'] ,y_prediction))
print('recall: ',metrics.recall_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

"""According to this feature selection method  to streamline model managed to reduce the feature count from 256 to 5 and 25

### sklearn.decomposition.PCA

Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.40,svd_solver='full')
pca.fit(X_train_scaled['label_1'])
x_train_transformation= pd.DataFrame(pca.transform(X_train_scaled['label_3']))
x_valid_transformation= pd.DataFrame(pca.transform(X_valid_scaled['label_3']))
x_test_transformation= pd.DataFrame(pca.transform(X_test_scaled['label_3']))
x_test_transformation_pass= pca.transform(X_test_scaled['label_3'])
print ('New Shape according to the feature selection technique 1', x_train_transformation.shape )

"""Then try to train the model with new feature count and see the difference."""

classification=svm.SVC(kernel='linear')
classification.fit(x_train_transformation,Y_train_scaled['label_3'])
y_prediction = classification.predict(x_valid_transformation)
y_pred_test_after = classification.predict(x_test_transformation)
print('Predicted labels after feature engineering:', y_pred_test_after)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_3'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

print ("precision",metrics.precision_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))
print ("accuracy",metrics.accuracy_score( Y_valid_scaled['label_3'] ,y_prediction))
print ("recall",metrics.recall_score( Y_valid_scaled['label_3'] ,y_prediction,average='weighted'))

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test_before,
    'Predicted labels after feature engineering': y_pred_test_after,
    'No. of new features': x_test_transformation_pass.shape[1]
})


for i in range(x_test_transformation_pass.shape[1]):
  output_df[f'New feature {i+1}'] = x_test_transformation_pass[:, i]

output_df.head()

output_df.to_csv(f"190282X_label_3.csv", index=False)

"""# Label 4

## Model Training
"""

from sklearn.neighbors import KNeighborsClassifier


# Create a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed

# Fit the KNN classifier to the training data
knn_classifier.fit(X_train_scaled['label_4'], Y_train_scaled['label_4'])

# Make predictions using the KNN classifier
y_prediction = knn_classifier.predict(X_valid_scaled['label_4'])
y_pred_test_before = knn_classifier.predict(X_test_scaled['label_4'])
print('Predicted labels before feature engineering:', y_pred_test_before)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_4'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_4'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))

"""## Applying Feature Selection Techniques

### sklearn.decomposition.PCA

Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.70,svd_solver='full')
pca.fit(X_train_scaled['label_4'])
x_train_transformation= pd.DataFrame(pca.transform(X_train_scaled['label_4']))
x_valid_transformation= pd.DataFrame(pca.transform(X_valid_scaled['label_4']))
x_test_transformation= pca.transform(X_test_scaled['label_4'])


print ('New Shape according to the feature selection technique 1', x_train_transformation.shape )

"""Then try to train the model with new feature count and see the difference."""

from sklearn.neighbors import KNeighborsClassifier



# Create a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed

# Fit the KNN classifier to the training data
knn_classifier.fit(x_train_transformation, Y_train_scaled['label_4'])

# Make predictions using the KNN classifier
y_prediction = knn_classifier.predict(x_valid_transformation)
y_pred_test_after = knn_classifier.predict(x_test_transformation)
print('Predicted labels after feature engineering:', y_pred_test_after)

from sklearn import metrics
print(metrics.confusion_matrix( Y_valid_scaled['label_4'] ,y_prediction))

print (metrics.precision_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))

print (metrics.accuracy_score( Y_valid_scaled['label_4'] ,y_prediction))

print (metrics.recall_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))

print('precision: ',metrics.precision_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))
print('accuracy: ',metrics.accuracy_score( Y_valid_scaled['label_4'] ,y_prediction))
print('recall: ',metrics.recall_score( Y_valid_scaled['label_4'] ,y_prediction,average='weighted'))

output_df = pd.DataFrame({
    'Predicted labels before feature engineering': y_pred_test_before,
    'Predicted labels after feature engineering': y_pred_test_after,
    'No. of new features': x_test_transformation.shape[1]
})


for i in range(x_test_transformation.shape[1]):
  output_df[f'New feature {i+1}'] = x_test_transformation[:, i]

output_df.head()

output_df.to_csv(f"190282X_label_4.csv", index=False)